{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "Wrong number or type of arguments for overloaded function 'Tagger_parse'.\n  Possible C/C++ prototypes are:\n    MeCab::Tagger::parse(MeCab::Model const &,MeCab::Lattice *)\n    MeCab::Tagger::parse(MeCab::Lattice *) const\n    MeCab::Tagger::parse(char const *)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-89d93b59fc3f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m   \u001b[0mdocuments\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_sentence2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmin_df\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-89d93b59fc3f>\u001b[0m in \u001b[0;36m_sentence2bow\u001b[1;34m(sentence)\u001b[0m\n\u001b[0;32m     37\u001b[0m   \u001b[1;31m# e.g. 動詞:surface=\"滑れ\", feature=\"動詞,自立,*,*,一段,未然形,滑れる,スベレ,スベレ\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#  for node in tagger.parse(sentence, as_nodes=True):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 39\u001b[1;33m   \u001b[1;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[0mfeatures\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\",\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"名詞\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\MeCab.py\u001b[0m in \u001b[0;36mparse\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 428\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    429\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    430\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: Wrong number or type of arguments for overloaded function 'Tagger_parse'.\n  Possible C/C++ prototypes are:\n    MeCab::Tagger::parse(MeCab::Model const &,MeCab::Lattice *)\n    MeCab::Tagger::parse(MeCab::Lattice *) const\n    MeCab::Tagger::parse(char const *)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#import mojimoji as mj\n",
    "#from natto import MeCab\n",
    "import MeCab as mc\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SQLContext, sql\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "#tagger = MeCab('-d /usr/local/lib/mecab/dic/mecab-ipadic-neologd')\n",
    "tagger = mc.Tagger()\n",
    "\n",
    "ng_noun = [\"これ\", \"よう\",  \"こと\", \"の\", \"もの\", \"それ\", \"とき\"] # お好みで\n",
    "\n",
    "appName = 'association'\n",
    "conf = pyspark.SparkConf().setAppName(appName).setMaster('local[4]').set(\"spark.executor.cores\", \"2\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "sqlContext = sql.SQLContext(sc)\n",
    "\n",
    "\n",
    "def _sentence2bow(sentence):\n",
    "  \"\"\"\n",
    "  文を形態素解析してBagOfWordsに変換\n",
    "  @param sentence: text\n",
    "    自然言語の文\n",
    "  @return bag: list\n",
    "    語形変化が修正された単語のリスト\n",
    "  \"\"\"\n",
    "  bag = []\n",
    "  # e.g. 動詞:surface=\"滑れ\", feature=\"動詞,自立,*,*,一段,未然形,滑れる,スベレ,スベレ\"\n",
    "#  for node in tagger.parse(sentence, as_nodes=True):\n",
    "  for node in tagger.parse(sentence):\n",
    "    features = node.feature.split(\",\")\n",
    "    if features[0] == \"名詞\":\n",
    "#      noun = mj.zen_to_han(node.surface.decode('utf-8')).encode('utf-8')\n",
    "        noun = node.surface\n",
    "    if noun not in ng_noun:\n",
    "        bag.append(node.surface)\n",
    "\n",
    "  # 文書中の重複はまとめてしまう\n",
    "  return list(set(bag))\n",
    "\n",
    "\n",
    "file = \"../text/test.txt\"\n",
    "df = pd.read_csv(file, delimiter='\\t', names=[\"URL\", \"Text\"],\n",
    "  dtype = {'URL':'object', 'Text':'object'})\n",
    "\n",
    "documents = []\n",
    "for i, row in df.iterrows():\n",
    "  documents.append(' '.join(_sentence2bow(row['Text'])))\n",
    "\n",
    "vectorizer = CountVectorizer(max_df=0.5, min_df=2, max_features=100)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "features = vectorizer.get_feature_names()\n",
    "\n",
    "colnames = ['doc' + str(i) for i in range(0, X.shape[0])]\n",
    "\n",
    "index = 'word'\n",
    "pdf = pd.DataFrame(X.T.toarray())\n",
    "pdf[index] = features\n",
    "\n",
    "def _createDataFrame(df, colnames, index):\n",
    "  idx = str(index)\n",
    "  col = [col + '_' + idx for col in colnames]\n",
    "  fields = [StructField(field_name, IntegerType(), True) for field_name in col]\n",
    "  fields.append(StructField(\"word\" + \"_\" + idx , StringType(), True))\n",
    "  sdf = sqlContext.createDataFrame(pdf, StructType(fields))\n",
    "  return sdf\n",
    "\n",
    "sdf1 = _createDataFrame(pdf, colnames, 1)\n",
    "sdf2 = _createDataFrame(pdf, colnames, 2)\n",
    "\n",
    "joined = sdf1.join(sdf2, sdf1.word_1 < sdf2.word_2)\n",
    "\n",
    "result = joined.rdd.map(lambda x: (\n",
    "    x[\"word_1\"],\n",
    "    x[\"word_2\"],\n",
    "    float(sum([min(x[c +'_1'], x[c + '_2']) for c in colnames])) /\n",
    "    float(sum([max(x[c +'_1'], x[c + '_2']) for c in colnames]))\n",
    "    )).filter(lambda x: x[2] > 0.01).collect()\n",
    "\n",
    "# build network \n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(features, size=10)\n",
    "\n",
    "# edgeの追加\n",
    "edge_threshold = 0.15\n",
    "for i, j, w in result:\n",
    "    if w > edge_threshold:\n",
    "        G.add_edge(i, j, weight=w)\n",
    "\n",
    "# 孤立したnodeを削除\n",
    "isolated = [n for n in G.nodes if len([ i for i in nx.all_neighbors(G, n)]) == 0]\n",
    "for n in isolated:\n",
    "    G.remove_node(n)\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "pos = nx.spring_layout(G, k=0.3) # k = node間反発係数\n",
    "\n",
    "# nodeの大きさ\n",
    "node_size = [d[\"size\"]*50 for (n,d) in G.nodes(data=True)]\n",
    "nx.draw_networkx_nodes(G, pos, node_color=\"b\",alpha=0.3, node_size=node_size)\n",
    "\n",
    "# 日本語ラベル\n",
    "nx.draw_networkx_labels(G, pos, fontsize=14, font_family=\"Hiragino Kaku Gothic Pro\", font_weight=\"bold\")\n",
    "\n",
    "# エッジの太さ調節\n",
    "edge_width = [ d[\"weight\"]*20 for (u,v,d) in G.edges(data=True)]\n",
    "nx.draw_networkx_edges(G, pos, alpha=0.4, edge_color=\"c\", width=edge_width)\n",
    "\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "\n",
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
