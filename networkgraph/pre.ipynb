{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gephiで共起ネットワークの可視化\n",
    "Mecabで形態素解析し、同じ文中で出現する[名詞、形容詞、動詞]をノードとする。\n",
    "\n",
    "共起関係がエッジ。\n",
    "\n",
    "抽出した単語を文ごとにカンマで区切り，CSVファイルを作成する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 分かち書きデータの用意janome\n",
    "\n",
    "from janome.tokenizer import Tokenizer\n",
    "from janome.analyzer import Analyzer\n",
    "from janome.charfilter import UnicodeNormalizeCharFilter, RegexReplaceCharFilter\n",
    "from janome.tokenfilter import POSStopFilter, LowerCaseFilter, ExtractAttributeFilter\n",
    "\n",
    "# 助詞などを除外した上で分かち書き\n",
    "def wakati_filter(text: str,\n",
    "                  char_reg_filter=[(\"[,\\.\\(\\)\\{\\}\\[\\]]\",\" \")],\n",
    "                  ignore_filter=['接続詞', '接頭辞', '接尾辞',  '助詞', '助動詞']):\n",
    "    \n",
    "    char_filters = [UnicodeNormalizeCharFilter(), # UnicodeをNFKCで正規化\n",
    "                    RegexReplaceCharFilter('\\d+', '0')] # 数字を全て0に置換\n",
    "    for reg in char_reg_filter:\n",
    "        char_filters.append(RegexReplaceCharFilter(*reg))\n",
    "\n",
    "    tokenizer = Tokenizer()\n",
    "\n",
    "    token_filters = [POSStopFilter(ignore_filter), # 除外対象を設定\n",
    "                     LowerCaseFilter(), # 英字は小文字に統一\n",
    "#                     ExtractAttributeFilter('base_form')  # 原型のみを取得\n",
    "                    ]\n",
    "\n",
    "    analyzer = Analyzer(char_filters, tokenizer, token_filters)\n",
    "    return [token.surface for token in analyzer.analyze(text)] # token.surfaceは表層形(語彙)\n",
    "\n",
    "# 頻出語カウント\n",
    "def countwords(textlist):\n",
    "    # counting\n",
    "    words = {}\n",
    "    for word in textlist:\n",
    "        words[word] = words.get(word, 0) + 1\n",
    "\n",
    "    # sort by count\n",
    "    d = [(v,k) for k,v in words.items()]\n",
    "    d.sort()\n",
    "    d.reverse()\n",
    "    for count, word in d[:50]:\n",
    "        print(count, word)\n",
    "        \n",
    "# 除去\n",
    "def remove_stopwords(words, stopwords):\n",
    "    words = [word for word in words if word not in stopwords]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "485 \n",
      "\n",
      "192 保険\n",
      "91 ai\n",
      "86 サービス\n",
      "74 業務\n",
      "66 開発\n",
      "66 活用\n",
      "66 データ\n",
      "66 システム\n",
      "61 提供\n",
      "57 技術\n",
      "52 導入\n",
      "47 企業\n",
      "44 情報\n",
      "40 生命\n",
      "39 顧客\n",
      "34 分析\n",
      "33 日本\n",
      "31 銀行\n",
      "31 認証\n",
      "31 自動\n",
      "31 管理\n",
      "31 実証\n",
      "30 商品\n",
      "30 効率\n",
      "29 連携\n",
      "29 人材\n",
      "29 ブロック\n",
      "29 チェーン\n",
      "28 可能\n",
      "27 実験\n",
      "27 健康\n",
      "26 スマート\n",
      "25 金融\n",
      "24 ジャパン\n",
      "24 rpa\n",
      "23 確認\n",
      "23 時間\n",
      "23 必要\n",
      "23 人\n",
      "23 三井\n",
      "22 音声\n",
      "22 採用\n",
      "22 アプリ\n",
      "21 証券\n",
      "21 育成\n",
      "21 支援\n",
      "21 専門\n",
      "21 対象\n",
      "21 nec\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# 分かち書きさせるファイルの読み込み\n",
    "filename = \"../text/test.txt\"\n",
    "text = open(filename, \"r\",encoding=\"utf-8\").read()\n",
    "\n",
    "# 分かち書き\n",
    "wakati_text = wakati_filter(text)\n",
    "\n",
    "# ストップワード\n",
    "#stopwords = get_stop_words(wakati_text)\n",
    "#print('stop words: {}'.format(stopwords))\n",
    "\n",
    "# 頻出語を確認して不要な語を指定(データをもながら増やす)\n",
    "\n",
    "stopwords = ['0', 'し', 'する', '化', '年', '月', '日',\n",
    "         'できる', 'こと', 'さ', 'いる', '者', '向け', '的', \n",
    "         '利用', '開始', '発表', '社','れ', '型', 'なる', '性', 'ら',\n",
    "         '加え', 'でき', '全', '行う', '円', 'おり', 'なっ', '会', '第','。','、','「','」',' ', '・']\n",
    "\n",
    "# 除去\n",
    "wakati_text = remove_stopwords(wakati_text, stopwords)\n",
    "print(countwords(wakati_text))\n",
    "# 分かち書き後のデータを出力\n",
    "file = open('wakati_text.csv', 'w',encoding=\"utf-8\")\n",
    "file.writelines(\" \".join(wakati_text))\n",
    "file.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
